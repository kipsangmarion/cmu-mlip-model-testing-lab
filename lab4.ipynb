{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cfc1c7",
   "metadata": {},
   "source": [
    "# Step 1 - Install the required dependencies, set up W&B and make sure the python version is 3.10 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1012c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb datasets transformers evaluate tqdm emoji regex pandas pyarrow scikit-learn\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a071bc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmkipsang\u001b[0m (\u001b[33mmkipsang-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "# wandb.login(key = \"\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f18b443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.8\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b8e44d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\STUDENT\\marionvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\STUDENT\\marionvenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#imports and config:\n",
    "import re, regex, emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# WANDB CONFIG\n",
    "PROJECT = \"mlip-lab4-slices-2025\"    \n",
    "ENTITY = None                        \n",
    "RUN_NAME = \"tweet_eval_roberta_vs_gpt2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3a6beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare\n",
    "MODELS = {\n",
    "    \"roberta\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    \"gpt2\":    \"LYTinn/finetuning-sentiment-model-tweet-gpt2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "434c2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Label normalization \n",
    "ID2LABEL = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "HF_LABEL_MAP = {\"LABEL_0\":\"negative\",\"LABEL_1\":\"neutral\",\"LABEL_2\":\"positive\"}\n",
    "\n",
    "USE_HF_DATASET = True   # set False to use tweets.csv fallback\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289dcac3",
   "metadata": {},
   "source": [
    "# Step 2 - Load a dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9575ae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\STUDENT\\marionvenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\STUDENT\\.cache\\huggingface\\hub\\datasets--cardiffnlp--tweet_eval. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45615/45615 [00:00<00:00, 713107.50 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12284/12284 [00:00<00:00, 1757558.60 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 483688.40 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user @user what do these '1/2 naked pics' hav...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH: ‚ÄúI had a blue penis while I was this‚Äù [pla...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user That's coming, but I think the vic...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  @user @user what do these '1/2 naked pics' hav...  neutral\n",
       "1  OH: ‚ÄúI had a blue penis while I was this‚Äù [pla...  neutral\n",
       "2  @user @user That's coming, but I think the vic...  neutral"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if USE_HF_DATASET:\n",
    "    ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "    df = pd.DataFrame(ds[\"test\"]).head(500).copy()\n",
    "    df[\"label\"] = df[\"label\"].map(ID2LABEL)\n",
    "else:\n",
    "    df = pd.read_csv(\"tweets.csv\")\n",
    "    # Ensure it has 'text' and 'label' columns\n",
    "    df = df.rename(columns={c: c.strip() for c in df.columns})\n",
    "    assert {\"text\",\"label\"}.issubset(df.columns), \"tweets.csv must include text,label\"\n",
    "\n",
    "df = df[[\"text\",\"label\"]].dropna().reset_index(drop=True)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0546f1",
   "metadata": {},
   "source": [
    "# Step 3 - Add MetaData for slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089146d4",
   "metadata": {},
   "source": [
    "In this step, you'll add **5 metadata columns** to your dataset to enable slicing later in **Weights & Biases (W&B)**.\n",
    "\n",
    "You can use:\n",
    "- **Value matching** (e.g., tweets with hashtags)\n",
    "- **Regex** (e.g., strong positive words like *love*, *great*)\n",
    "- **Heuristics** (e.g., emoji count, all-caps detection, tweet length)\n",
    "\n",
    "These columns will be carried forward when you run inference in Step 6 and will appear in your final `predictions_table` logged to W&B.\n",
    "\n",
    "---\n",
    "\n",
    "Once inference is complete, your W&B table (`df_long`) will include:\n",
    "- Original tweet text\n",
    "- Ground-truth labels\n",
    "- Model predictions and confidence scores\n",
    "- All slicing metadata you define here\n",
    "\n",
    "Later, in the W&B UI, you can use the ‚ûï `Filter` option in the table view to explore model behavior across these slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fd5775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Step 3 ‚Äì Add Slicing Metadata\n",
    "# Add new columns for filtering in W&B later\n",
    "\n",
    "# Example: count emojis in each tweet & create a slice for tweets with >3 emojis\n",
    "def count_emojis(text):\n",
    "    return sum(ch in emoji.EMOJI_DATA for ch in str(text))\n",
    "\n",
    "df[\"emoji_count\"] = df[\"text\"].apply(count_emojis).astype(int)\n",
    "\n",
    "NEGATION_RE = re.compile(r\"\\b(?:no|not|never|nothing|nowhere|hardly|scarcely|barely|n't)\\b\", re.I)\n",
    "def has_negation(s: str) -> bool:\n",
    "    return bool(NEGATION_RE.search(str(s)))\n",
    "\n",
    "URL_RE      = re.compile(r\"(https?://|www\\.)\", re.I)\n",
    "def has_url_or_mention(s: str) -> bool:\n",
    "    s = str(s)\n",
    "    return bool(URL_RE.search(s) or MENTION_RE.search(s))\n",
    "\n",
    "MENTION_RE  = re.compile(r\"@\\w+\")\n",
    "def is_long(s: str, n: int = 100) -> bool:\n",
    "    return len(str(s)) > n\n",
    "\n",
    "def get_slices(df):\n",
    "    return {\n",
    "        \"emoji_gt3\": df[\"emoji_count\"] > 3,\n",
    "        \"has_negation\":        df[\"text\"].apply(has_negation),\n",
    "        \"long_gt100chars\":     df[\"text\"].apply(is_long),\n",
    "        \"has_url_or_mention\":  df[\"text\"].apply(has_url_or_mention),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a75f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0+cpu\n",
      "transformers: 4.56.1\n",
      "CUDA available: False\n",
      "Python: c:\\Users\\STUDENT\\marionvenv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# Transformers requires a backend (PyTorch/TensorFlow/Flax). We'll use PyTorch.\n",
    "try:\n",
    "    import torch, transformers, sys\n",
    "    print(\"torch:\", torch.__version__)\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"Python:\", sys.executable)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install PyTorch before proceeding: pip install torch torchvision torchaudio\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ec371",
   "metadata": {},
   "source": [
    "#  Step 4 ‚Äì Run Inference on Tweets Using Two Sentiment Models\n",
    "\n",
    "In this step, you'll use two HuggingFace sentiment analysis models to run inference on your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89f69d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Infer: cardiffnlp/twitter-roberta-base-sentiment-latest:   0%|          | 0/500 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Infer: cardiffnlp/twitter-roberta-base-sentiment-latest: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:34<00:00, 14.42it/s]\n",
      "Device set to use cpu\n",
      "Infer: LYTinn/finetuning-sentiment-model-tweet-gpt2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:32<00:00, 15.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>model</th>\n",
       "      <th>pred</th>\n",
       "      <th>conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user @user what do these '1/2 naked pics' hav...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>roberta</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.804726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH: ‚ÄúI had a blue penis while I was this‚Äù [pla...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>roberta</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.866949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user That's coming, but I think the vic...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>roberta</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.763724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think I may be finally in with the in crowd ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>roberta</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.774047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user Wow,first Hugo Chavez and now Fidel Cast...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>roberta</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.416397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label  emoji_count  \\\n",
       "0  @user @user what do these '1/2 naked pics' hav...   neutral            0   \n",
       "1  OH: ‚ÄúI had a blue penis while I was this‚Äù [pla...   neutral            0   \n",
       "2  @user @user That's coming, but I think the vic...   neutral            0   \n",
       "3  I think I may be finally in with the in crowd ...  positive            0   \n",
       "4  @user Wow,first Hugo Chavez and now Fidel Cast...  negative            0   \n",
       "\n",
       "     model      pred      conf  \n",
       "0  roberta  negative  0.804726  \n",
       "1  roberta   neutral  0.866949  \n",
       "2  roberta   neutral  0.763724  \n",
       "3  roberta  positive  0.774047  \n",
       "4  roberta   neutral  0.416397  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_pipeline(model_id, texts):\n",
    "    clf = pipeline(\"text-classification\", model=model_id, truncation=True, framework=\"pt\", device=-1)\n",
    "    preds, confs = [], []\n",
    "    for t in tqdm(texts, desc=f\"Infer: {model_id}\"):\n",
    "        out = clf(t)[0]\n",
    "        lbl = HF_LABEL_MAP.get(out[\"label\"], out[\"label\"])\n",
    "        preds.append(lbl)\n",
    "        confs.append(float(out[\"score\"]))\n",
    "    return preds, confs\n",
    "\n",
    "pred_frames = []\n",
    "for model_name, model_id in MODELS.items():\n",
    "    yhat, conf = run_pipeline(model_id, df[\"text\"].tolist())\n",
    "    tmp = df.copy()\n",
    "    tmp[\"model\"] = model_name\n",
    "    tmp[\"pred\"]  = yhat\n",
    "    tmp[\"conf\"]  = conf\n",
    "    pred_frames.append(tmp)\n",
    "\n",
    "df_long = pd.concat(pred_frames, ignore_index=True)\n",
    "df_long.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dedaa9",
   "metadata": {},
   "source": [
    "# Step 5: Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b77d0e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\STUDENT\\AppData\\Local\\Temp\\ipykernel_43888\\1664234989.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: compute_accuracy(g[\"label\"], g[\"pred\"]))\n"
     ]
    }
   ],
   "source": [
    "#compute metrics model-wise\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    y_true = list(y_true)\n",
    "    y_pred = list(y_pred)\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "overall = (\n",
    "    df_long.groupby(\"model\")\n",
    "           .apply(lambda g: compute_accuracy(g[\"label\"], g[\"pred\"]))\n",
    ")\n",
    "\n",
    "slice_table = wandb.Table(columns=[\"slice\", \"model\", \"accuracy\"])\n",
    "slice_metrics = {}\n",
    "\n",
    "for slice_name, mask in get_slices(df_long).items():\n",
    "    slice_metrics[slice_name] = {}  # Initialize inner dict\n",
    "\n",
    "    for model_name, g in df_long[mask].groupby(\"model\"):\n",
    "        acc = compute_accuracy(g[\"label\"], g[\"pred\"])\n",
    "        acc = float(acc) \n",
    "        # Add to wandb Table\n",
    "        slice_table.add_data(slice_name, model_name, acc)\n",
    "        # Add to dict\n",
    "        slice_metrics[slice_name][model_name] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7843a",
   "metadata": {},
   "source": [
    "# Step 6: Log to Wandb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "425dd4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\STUDENT\\Documents\\Fall2025\\MLinProd\\cmu-mlip-model-testing-lab\\wandb\\run-20250919_153458-0lm7pbqg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkipsang-carnegie-mellon-university/mlip-lab4-slices-2025/runs/0lm7pbqg' target=\"_blank\">tweet_eval_roberta_vs_gpt2</a></strong> to <a href='https://wandb.ai/mkipsang-carnegie-mellon-university/mlip-lab4-slices-2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkipsang-carnegie-mellon-university/mlip-lab4-slices-2025' target=\"_blank\">https://wandb.ai/mkipsang-carnegie-mellon-university/mlip-lab4-slices-2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkipsang-carnegie-mellon-university/mlip-lab4-slices-2025/runs/0lm7pbqg' target=\"_blank\">https://wandb.ai/mkipsang-carnegie-mellon-university/mlip-lab4-slices-2025/runs/0lm7pbqg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>slice/emoji_gt3/gpt2_accuracy</td><td>‚ñÅ</td></tr><tr><td>slice/emoji_gt3/roberta_accuracy</td><td>‚ñÅ</td></tr><tr><td>slice/has_negation/gpt2_accuracy</td><td>‚ñÅ</td></tr><tr><td>slice/has_negation/roberta_accuracy</td><td>‚ñÅ</td></tr><tr><td>slice/has_url_or_mention/gpt2_accuracy</td><td>‚ñÅ</td></tr><tr><td>slice/has_url_or_mention/roberta_accuracy</td><td>‚ñÅ</td></tr><tr><td>slice/long_gt100chars/gpt2_accuracy</td><td>‚ñÅ</td></tr><tr><td>slice/long_gt100chars/roberta_accuracy</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpt2_accuracy</td><td>0.398</td></tr><tr><td>roberta_accuracy</td><td>0.698</td></tr><tr><td>slice/emoji_gt3/gpt2_accuracy</td><td>1</td></tr><tr><td>slice/emoji_gt3/roberta_accuracy</td><td>0</td></tr><tr><td>slice/has_negation/gpt2_accuracy</td><td>0.33333</td></tr><tr><td>slice/has_negation/roberta_accuracy</td><td>0.66667</td></tr><tr><td>slice/has_url_or_mention/gpt2_accuracy</td><td>0.29279</td></tr><tr><td>slice/has_url_or_mention/roberta_accuracy</td><td>0.68018</td></tr><tr><td>slice/long_gt100chars/gpt2_accuracy</td><td>0.33486</td></tr><tr><td>slice/long_gt100chars/roberta_accuracy</td><td>0.66055</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tweet_eval_roberta_vs_gpt2</strong> at: <a href='https://wandb.ai/mkipsang-carnegie-mellon-university/mlip-lab4-slices-2025/runs/0lm7pbqg' target=\"_blank\">https://wandb.ai/mkipsang-carnegie-mellon-university/mlip-lab4-slices-2025/runs/0lm7pbqg</a><br> View project at: <a href='https://wandb.ai/mkipsang-carnegie-mellon-university/mlip-lab4-slices-2025' target=\"_blank\">https://wandb.ai/mkipsang-carnegie-mellon-university/mlip-lab4-slices-2025</a><br>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250919_153458-0lm7pbqg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=PROJECT, entity=ENTITY, name=RUN_NAME, config={\n",
    "    \"models\": MODELS,\n",
    "    \"n_rows\": len(df),\n",
    "    \"use_hf_dataset\": USE_HF_DATASET\n",
    "})\n",
    "\n",
    "# Main predictions table: one row per (example, model)\n",
    "pred_table = wandb.Table(dataframe=df_long)\n",
    "wandb.log({\"predictions_table\": pred_table})\n",
    "\n",
    "# Log overall accuracy to wandb summary\n",
    "for model_name, acc in overall.items():\n",
    "    wandb.summary[f\"{model_name}_accuracy\"] = float(acc)\n",
    "\n",
    "# wandb.log({\"slice_accuracy_table\": slice_table})\n",
    "for slice_name, model_dict in slice_metrics.items():\n",
    "    for model_name, acc in model_dict.items():\n",
    "        metric_name = f\"slice/{slice_name}/{model_name}_accuracy\"\n",
    "        wandb.log({metric_name: acc})\n",
    "\n",
    "\n",
    "wandb.log({\"slice_metrics\": slice_table})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c3b74",
   "metadata": {},
   "source": [
    "## Instructions: Exploring Slice-Based Evaluation in W&B\n",
    " Step 1: Open the W&B Project\n",
    "- Click on the **project link** above.\n",
    "- Click on the **latest run** near the top.\n",
    "Step 2: View Tables\n",
    "- Click the **\"Tables\"** tab.\n",
    "- You should see:\n",
    "  - `predictions_table`\n",
    "  - `slice_metrics`\n",
    "\n",
    "Step 3: Use Filters in `predictions_table`\n",
    "- Click on `predictions_table`.\n",
    "- Use the filter bar to explore:\n",
    "Example (see image):\n",
    "  ```python\n",
    "  col2 == 0\n",
    "Step 4: \n",
    "- Check slice_metrics table: It shows accuracy of each model for every slice.\n",
    "- Add a Bar Chart Panel: Click the \"Add panels\" button (top-right).\n",
    "- Choose Bar chart under \"Charts\".\n",
    "Try to create bar charts comparing accuracies of both models for a slice. Do it for 2 slices.\n",
    "\n",
    "Discuss your findings with your TA.\n",
    "\n",
    "# Filtering: \n",
    "<img src=\"images/filtering.png\" alt=\"Predictions Table\" width=\"600\">\n",
    "\n",
    "## Plotting:\n",
    "<img src=\"images/plotting.png\" alt=\"Predictions Table\" height=\"300\">\n",
    "<img src=\"images/bar-charts.png\" alt=\"Predictions Table\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41f83c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Globally, RoBERTa achieved around 0.68 accurac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Slice analysis revealed that RoBERTa struggled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On the negation slice, RoBERTa achieved around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For tweets containing URLs or @mentions, RoBER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On longer tweets (&gt;100 characters), RoBERTa ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Globally, RoBERTa achieved around 0.68 accurac...\n",
       "1  Slice analysis revealed that RoBERTa struggled...\n",
       "2  On the negation slice, RoBERTa achieved around...\n",
       "3  For tweets containing URLs or @mentions, RoBER...\n",
       "4  On longer tweets (>100 characters), RoBERTa ma..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Students: replace the placeholders below with 1‚Äì2 sentence insights\n",
    "saved_slice_notes = [\n",
    "    \"Globally, RoBERTa achieved around 0.68 accuracy while GPT-2 lagged at around 0.40, showing that RoBERTa is much more reliable for tweet sentiment.\",\n",
    "    \"Slice analysis revealed that RoBERTa struggled heavily on emoji-rich tweets\",\n",
    "    \"On the negation slice, RoBERTa achieved around 0.65 accuracy compared to GPT-2 at around 0.35, showing that GPT-2 struggles much more with polarity flips like 'not good'\",\n",
    "    \"For tweets containing URLs or @mentions, RoBERTa again performed strongly ( around 0.66) while GPT-2 lagged at around 0.30, suggesting RoBERTa handles noisy social media text better\",\n",
    "    \"On longer tweets (>100 characters), RoBERTa maintained around 0.65 accuracy while GPT-2 dropped to around 0.35, showing GPT-2 is less robust to extended context\"\n",
    "]\n",
    "pd.DataFrame(saved_slice_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3039bf2",
   "metadata": {},
   "source": [
    "\n",
    "After successfully creating the two slices, come up with three *additional* slices you want to check and **create** the slices & view them in Wandb.\n",
    "\n",
    "There are two directions to identify useful slices:\n",
    "- Top-down: Think about what kinds of things the model can struggle with, and come up with some slices.\n",
    "- Bottom-up: Look at model (mis-)predictions, come up with hypotheses, and translate them into data slices.\n",
    "\n",
    "3. Tweets containing negation words\n",
    "4. Tweets longer than 100 characters to test whether truncation or length impacts accuracy\n",
    "5. Tweets containing URLs or @mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3345162b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Tweets containing negation words (e.g., 'not', 'never') since models often misinterpret sentiment flips.\",\n",
       " 'Tweets longer than 100 characters to test whether truncation or length impacts accuracy.',\n",
       " 'Tweets containing URLs or @mentions, since models may struggle to parse external references or usernames.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add these three slices & re-run the notebook to see them on Wandb.\n",
    "\n",
    "additional_slice_ideas = [\n",
    "    \"Tweets containing negation words (e.g., 'not', 'never') since models often misinterpret sentiment flips.\",\n",
    "    \"Tweets longer than 100 characters to test whether truncation or length impacts accuracy.\",\n",
    "    \"Tweets containing URLs or @mentions, since models may struggle to parse external references or usernames.\"\n",
    "]\n",
    "additional_slice_ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e58835",
   "metadata": {},
   "source": [
    "# Step 7 - Write down three addition data slices you want to create but do not have the metadata for slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2b21d",
   "metadata": {},
   "source": [
    "In the previous step, you might have already come up with some slices you wanted to create but found it hard to do with existing metadata. Write down three of such slices in this step.\n",
    "\n",
    "Example: \n",
    "- I want to create a slice on tweets using slangs\n",
    "- I want to create a slice on non-English tweets (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da135c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Tweets containing slang expressions (e.g., 'lol', 'brb', 'smh'), since models might not understand informal language\",\n",
       " 'Tweets written in non-English languages, which may confuse models trained primarily on English text',\n",
       " \"Tweets with sarcasm indicators like 'yeah right' or 'totally', since sentiment can be opposite of literal meaning\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Write down three additional data slices here:\n",
    "\n",
    "additional_slice_descriptions = [\n",
    "    \"Tweets containing slang expressions (e.g., 'lol', 'brb', 'smh'), since models might not understand informal language\",\n",
    "    \"Tweets written in non-English languages, which may confuse models trained primarily on English text\",\n",
    "    \"Tweets with sarcasm indicators like 'yeah right' or 'totally', since sentiment can be opposite of literal meaning\"\n",
    "]\n",
    "additional_slice_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa906c",
   "metadata": {},
   "source": [
    "# Step 8 - Generate more test cases with Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396331d",
   "metadata": {},
   "source": [
    "Select one slice from the three you wrote down and generate **10 test cases** using LLMs, which can include average case, boundary case, or difficult case.\n",
    "\n",
    "Your input can be in the following format:\n",
    "\n",
    "> Examples:\n",
    "> - @user @user That‚Äôs coming, but I think the victims are going to be Medicaid recipients.\n",
    "> - I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
    "> \n",
    "> Generate more tweets using slangs.\n",
    "\n",
    "The first part of **Examples** conditions the LLM on the style, length, and content of examples. The second part of **Instructions** instructs what kind of examples you want LLM to generate.\n",
    "\n",
    "Use our provided GPTs to start the task: [llm-based-test-case-generator](https://chatgpt.com/g/g-982cylVn2-llm-based-test-case-generator). If you do not have access to GPTs, use the plain ChatGPT or other LLM providers you have access to instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49623362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your 10 generated tweets here:\n",
    "generated_slice_description = \"Examples: \" \\\n",
    "\"- lol that movie was trash, can't believe I wasted 2 hours smh \" \\\n",
    "\"- brb gotta go deal with this drama, life is wild rn \" \\\n",
    "\"Generate 10 more tweets using modern slang expressions.\"\n",
    "\n",
    "generated_cases = [\n",
    "    \"not me crying over a tiktok I've seen 10 times already üò≠\",\n",
    "    \"this wifi moving like it's powered by vibes only üò§\",\n",
    "    \"tell me why my food delivery said ‚Äúout for delivery‚Äù 40 mins ago‚Ä¶ I'm starvingggg\",\n",
    "    \"woke up and chose chaos today, sorry not sorry\",\n",
    "    \"y'all ever get hit with that ‚Äúwe need to talk‚Äù text and immediately start spiraling?? üíÄ\",\n",
    "    \"it's giving broke, it's giving stress, it's giving me\",\n",
    "    \"someone said my outfit was ‚Äúinteresting‚Äù and now I'm spiraling, thanks\",\n",
    "    \"had one iced coffee and now I think I can conquer the world\",\n",
    "    \"these vibes? immaculate. this playlist? undefeated.\",\n",
    "    \"I was today years old when I found out you‚Äôre supposed to rinse rice before cooking üò≥\"\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (marionvenv)",
   "language": "python",
   "name": "marionvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
