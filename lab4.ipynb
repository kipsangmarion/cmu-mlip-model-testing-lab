{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cfc1c7",
   "metadata": {},
   "source": [
    "# Step 1 - Install the required dependencies, set up W&B and make sure the python version is 3.10 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1012c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb datasets transformers evaluate tqdm emoji regex pandas pyarrow scikit-learn\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262b991",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a071bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.login(key = \"\")\n",
    "wandb.login(key=\"XX\") #replace with your key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and config:\n",
    "import re, regex, emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# WANDB CONFIG\n",
    "PROJECT = \"mlip-lab4-slices-2025\"    \n",
    "ENTITY = None                        \n",
    "RUN_NAME = \"tweet_eval_roberta_vs_gpt2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare\n",
    "MODELS = {\n",
    "    \"roberta\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    \"gpt2\":    \"LYTinn/finetuning-sentiment-model-tweet-gpt2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Label normalization \n",
    "ID2LABEL = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "HF_LABEL_MAP = {\"LABEL_0\":\"negative\",\"LABEL_1\":\"neutral\",\"LABEL_2\":\"positive\"}\n",
    "\n",
    "USE_HF_DATASET = True   # set False to use tweets.csv fallback\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289dcac3",
   "metadata": {},
   "source": [
    "# Step 2 - Load a dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HF_DATASET:\n",
    "    ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "    df = pd.DataFrame(ds[\"test\"]).head(500).copy()\n",
    "    df[\"label\"] = df[\"label\"].map(ID2LABEL)\n",
    "else:\n",
    "    df = pd.read_csv(\"tweets.csv\")\n",
    "    # Ensure it has 'text' and 'label' columns\n",
    "    df = df.rename(columns={c: c.strip() for c in df.columns})\n",
    "    assert {\"text\",\"label\"}.issubset(df.columns), \"tweets.csv must include text,label\"\n",
    "\n",
    "df = df[[\"text\",\"label\"]].dropna().reset_index(drop=True)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0546f1",
   "metadata": {},
   "source": [
    "# Step 3 - Add MetaData for slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089146d4",
   "metadata": {},
   "source": [
    "In this step, you'll add **5 metadata columns** to your dataset to enable slicing later in **Weights & Biases (W&B)**.\n",
    "\n",
    "You can use:\n",
    "- **Value matching** (e.g., tweets with hashtags)\n",
    "- **Regex** (e.g., strong positive words like *love*, *great*)\n",
    "- **Heuristics** (e.g., emoji count, all-caps detection, tweet length)\n",
    "\n",
    "These columns will be carried forward when you run inference in Step 6 and will appear in your final `predictions_table` logged to W&B.\n",
    "\n",
    "---\n",
    "\n",
    "Once inference is complete, your W&B table (`df_long`) will include:\n",
    "- Original tweet text\n",
    "- Ground-truth labels\n",
    "- Model predictions and confidence scores\n",
    "- All slicing metadata you define here\n",
    "\n",
    "Later, in the W&B UI, you can use the ➕ `Filter` option in the table view to explore model behavior across these slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd5775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 – Add Slicing Metadata\n",
    "# Add new columns for filtering in W&B later\n",
    "\n",
    "# Example: count emojis in each tweet\n",
    "def count_emojis(text):\n",
    "    return sum(ch in emoji.EMOJI_DATA for ch in str(text))\n",
    "\n",
    "df[\"emoji_count\"] = df[\"text\"].apply(count_emojis).astype(int)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a75f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers requires a backend (PyTorch/TensorFlow/Flax). We'll use PyTorch.\n",
    "try:\n",
    "    import torch, transformers, sys\n",
    "    print(\"torch:\", torch.__version__)\n",
    "    print(\"transformers:\", transformers.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"Python:\", sys.executable)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install PyTorch before proceeding: pip install torch torchvision torchaudio\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ec371",
   "metadata": {},
   "source": [
    "#  Step 4 – Run Inference on Tweets Using Two Sentiment Models\n",
    "\n",
    "In this step, you'll use two HuggingFace sentiment analysis models to run inference on your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f69d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_pipeline(model_id, texts):\n",
    "    clf = pipeline(\"text-classification\", model=model_id, truncation=True, framework=\"pt\", device=-1)\n",
    "    preds, confs = [], []\n",
    "    for t in tqdm(texts, desc=f\"Infer: {model_id}\"):\n",
    "        out = clf(t)[0]\n",
    "        lbl = HF_LABEL_MAP.get(out[\"label\"], out[\"label\"])\n",
    "        preds.append(lbl)\n",
    "        confs.append(float(out[\"score\"]))\n",
    "    return preds, confs\n",
    "\n",
    "pred_frames = []\n",
    "for model_name, model_id in MODELS.items():\n",
    "    yhat, conf = run_pipeline(model_id, df[\"text\"].tolist())\n",
    "    tmp = df.copy()\n",
    "    tmp[\"model\"] = model_name\n",
    "    tmp[\"pred\"]  = yhat\n",
    "    tmp[\"conf\"]  = conf\n",
    "    pred_frames.append(tmp)\n",
    "\n",
    "df_long = pd.concat(pred_frames, ignore_index=True)\n",
    "df_long.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dedaa9",
   "metadata": {},
   "source": [
    "# Step 5: Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics_sklearn(y_true, y_pred, average=\"macro\"):\n",
    "    y_true = list(y_true); y_pred = list(y_pred)\n",
    "    return {\n",
    "        \"accuracy\":  accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, average=average, zero_division=0),\n",
    "        \"recall\":    recall_score(y_true, y_pred, average=average, zero_division=0),\n",
    "        \"f1\":        f1_score(y_true, y_pred, average=average, zero_division=0),\n",
    "    }\n",
    "\n",
    "overall = (\n",
    "    df_long.groupby(\"model\")\n",
    "           .apply(lambda g: compute_metrics_sklearn(g[\"label\"], g[\"pred\"]))\n",
    "           .apply(pd.Series)\n",
    ")\n",
    "overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7843a",
   "metadata": {},
   "source": [
    "# Step 6: Log to Wandb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425dd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=PROJECT, entity=ENTITY, name=RUN_NAME, config={\n",
    "    \"models\": MODELS,\n",
    "    \"n_rows\": len(df),\n",
    "    \"use_hf_dataset\": USE_HF_DATASET\n",
    "})\n",
    "\n",
    "# Main predictions table: one row per (example, model)\n",
    "pred_table = wandb.Table(dataframe=df_long)\n",
    "wandb.log({\"predictions_table\": pred_table})\n",
    "\n",
    "# Log overall metrics per model into run summary\n",
    "for m, row in overall.iterrows():\n",
    "    for k, v in row.items():\n",
    "        wandb.summary[f\"{m}_{k}\"] = float(v)\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c3b74",
   "metadata": {},
   "source": [
    "Students now go to W&B → open the run → Tables → predictions_table.\n",
    "Use “+ Add filter” to create slices (e.g., has_hashtag is True, token_count < 8, emoji_count > 0, has_url is True, strong_neg_lexicon is True). They can add panels and apply different filters on each panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f83c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students: replace the placeholders below with 1–2 sentence insights\n",
    "saved_slice_notes = [\"\"\n",
    "]\n",
    "pd.DataFrame(saved_slice_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3039bf2",
   "metadata": {},
   "source": [
    "\n",
    "After successfully creating the two slices, come up with three *additional* slices you want to check and **create** the slices & view them in Wandb.\n",
    "\n",
    "There are two directions to identify useful slices:\n",
    "- Top-down: Think about what kinds of things the model can struggle with, and come up with some slices.\n",
    "- Bottom-up: Look at model (mis-)predictions, come up with hypotheses, and translate them into data slices.\n",
    "\n",
    "3. [YOUR CHOICE]\n",
    "4. [YOUR CHOICE]\n",
    "5. [YOUR CHOICE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these three slices & re-run the notebook to see them on Wandb.\n",
    "\n",
    "additional_slice_ideas = [\n",
    "\"\"\n",
    "]\n",
    "additional_slice_ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa906c",
   "metadata": {},
   "source": [
    "# Step 7 - Generate more test cases with Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396331d",
   "metadata": {},
   "source": [
    "Select one slice from the three you wrote down and generate **10 test cases** using LLMs, which can include average case, boundary case, or difficult case.\n",
    "\n",
    "Your input can be in the following format:\n",
    "\n",
    "> Examples:\n",
    "> - @user @user That’s coming, but I think the victims are going to be Medicaid recipients.\n",
    "> - I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
    "> \n",
    "> Generate more tweets using slangs.\n",
    "\n",
    "The first part of **Examples** conditions the LLM on the style, length, and content of examples. The second part of **Instructions** instructs what kind of examples you want LLM to generate.\n",
    "\n",
    "Use our provided GPTs to start the task: [llm-based-test-case-generator](https://chatgpt.com/g/g-982cylVn2-llm-based-test-case-generator). If you do not have access to GPTs, use the plain ChatGPT or other LLM providers you have access to instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49623362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your 10 generated tweets here:\n",
    "generated_slice = \"Sarcasm about products\"\n",
    "\n",
    "generated_cases = [\n",
    "    \"Love how my new laptop dies after 30 minutes. Revolutionary battery life.\",\n",
    "    \"This update made my phone faster — at crashing.\",\n",
    "    \"Amazing headphones: premium static included.\",\n",
    "    \"Customer support put me on hold just long enough to meditate. Namaste.\",\n",
    "    \"Five stars to the delivery for inventing ‘lost in transit’.\",\n",
    "    \"My smartwatch is so smart it forgot the time.\",\n",
    "    \"Great blender — now I own a cup of chunks.\",\n",
    "    \"Thanks to the camera update, I can fi/nally take abstract photos.\",\n",
    "    \"Alarm app works perfectly, except in the morning.\",\n",
    "    \"Truly premium earbuds: they pair with everyone except me.\",\n",
    "]\n",
    "\n",
    "gen_df = pd.DataFrame({\"text\": generated_cases})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce940d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both models on generated cases\n",
    "rows = []\n",
    "for model_name, model_id in MODELS.items():\n",
    "    yhat, conf = run_pipeline(model_id, gen_df[\"text\"].tolist())\n",
    "    tmp = gen_df.copy()\n",
    "    tmp[\"model\"] = model_name\n",
    "    tmp[\"pred\"]  = yhat\n",
    "    tmp[\"conf\"]  = conf\n",
    "    rows.append(tmp)\n",
    "\n",
    "gen_long = pd.concat(rows, ignore_index=True)\n",
    "gen_long[\"slice\"] = generated_slice\n",
    "gen_long.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aieng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
